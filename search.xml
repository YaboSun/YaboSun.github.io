<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[VS2017+MySQL8.0+MySQLconnector连接数据库]]></title>
    <url>%2F2018%2F09%2F04%2FVS2017%2BMySQL8.0%2BMySQLconnector%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[参考：官网VisualStudio连接MySQL教程使用MySQL connector/C++链接MySQL数据库C++ API方式连接mysql数据库实现增删改查C++连接mysql数据库的两种方法【C++】VS2015/VS2017连接Mysql数据库教程 实验室需要使用C++进行数据库应用开发，现将MySQL连接的过程记录如下： 环境搭建Windows10、VS2017、MySQL8.0、MySQL Connector/C++ 1.1.11、boost1.67.0 1、创建学生管理系统数据库MySQL数据库的安装过程这里不详细介绍，具体参考win10系统，mysql-installer-community-5.7.21.msi安装安装好MySQLServer以后，在控制台中创建数据库，输入 1create database university; 这里为了简单测试，只是创建一张学生表，创建的表如下： 1234567create table student( Sno int , Sname varchar (50), Ssex varchar(2), Sage int , primary key (Sno) ); 插入数据：为简单起见，这里只插入俩条数据12insert into student(Sno, Sname, Ssex, Sage) values(20081001197,'李子聪','M',17);insert into student(Sno, Sname, Ssex, Sage) values(20081001266,'蔡景学','F',19); 创建好的结果如下：在数据库命令行查看： 1select * from student; 项目配置配置依赖前，先将默认的Debug模式改为Release模式1、新建TestMySQLAPP 控制台项目这里或者直接新建一个空项目也可以2、依赖配置 1右键项目-&gt;属性-&gt;VC++目录 在包含目录点击编辑将已经下载解压好的Connector C++ 1.1.11 的\include 文件夹添加进去比如我的解压包路径是 1C:\Program Files\MySQL\Conector C++ 1.1.11\mysql-connector-c++-noinstall-1.1.11-winx64\include 以同样的方式将boost1.67.0 文件目录包含我的解压包路径是： 1D:\Boost1.67.0\boost_1_67_0 这里需要说一下为什么需要将boost 库包含进去，开始的时候我没有包含这部分，但是在运行过程中在mysql_connection.h 文件中又引用了这个库，我开始想着直接把其中引用部分的代码路径改一下，让其指向boost包对应的路径，结果发现修改后没有保存权限，所以还是老老实实将这个库目录包含进去配置好以上的以后接下来将Connector/C++ 的lib包含进去，还是在项目的属性页 1链接器-&gt;常规-&gt;附加库目录 以同样的方式添加，比如我的路径是 1C:\Program Files\MySQL\Conector C++ 1.1.11\mysql-connector-c++-noinstall-1.1.11-winx64\lib 添加以后的结果如下：接着在链接器-&gt;输入 部分将mysqlcppconn.lib 添加到这里基本上就将所有的配置完成，但是在运行过程中还是出现了一些错误，这部分没有截图，所以直接将解决的方法记录，参考的文章不记得是哪些博客，这里表示感谢简单来说就是需要将对应的一些动态库添加到C:\Windows\System32 目录中，这里我不太清楚为什么会出现错误，按照我的想法是在之前的配置中已经把Connector 的lib 全部添加到了项目中，为什么这里还是要把一些动态的库文件放到C:\Windows\System32 目录，但是确实结果是解决了问题。找到Connector/C++ 解压包路径，比如我的是 1C:\Program Files\MySQL\Conector C++ 1.1.11\mysql-connector-c++-noinstall-1.1.11-winx64\lib 将其中的.dll 文件全部拷贝粘贴到C:\Windows\System32 目录，然后运行即可 测试连接环境已经搭建完成，接下来测试一些简单的功能，在TestMySQLAPP.cpp 文件中这个文件其实就是你创建空项目的Main.cpp ，也就是程序的入口，添加以下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;stdafx.h&gt;#include &lt;stdlib.h&gt;#include &lt;iostream&gt;#include "mysql_connection.h"#include &lt;cppconn/driver.h&gt;#include &lt;cppconn/exception.h&gt;#include &lt;cppconn/resultset.h&gt;#include &lt;cppconn/statement.h&gt;using namespace std;int main()&#123; cout &lt;&lt; endl; cout &lt;&lt; "正在执行 SELECT sage from student where sno = '20081001197'" &lt;&lt; endl; try &#123; sql::Driver *driver; sql::Connection *con; sql::Statement *stmt; sql::ResultSet *res; /* 创建连接 */ driver = get_driver_instance(); con = driver-&gt;connect("tcp://127.0.0.1:3306", "root", "root"); // 需要修改为你数据库的用户密码 /* 连接 MySQL 数据库 university */ con-&gt;setSchema("university"); stmt = con-&gt;createStatement(); res = stmt-&gt;executeQuery("SELECT sage from student where sno = '20081001197'"); // 标准sql语句 while (res-&gt;next()) &#123; cout &lt;&lt; "\t MySQL replies: "; /* 通过数字偏移量, 1 代表第一列 */ cout &lt;&lt; res-&gt;getInt(1)&lt;&lt; endl; &#125; delete res; delete stmt; delete con; &#125; catch (sql::SQLException &amp;e) &#123; cout &lt;&lt; "# ERR: SQLException in " &lt;&lt; __FILE__; cout &lt;&lt; "(" &lt;&lt; __FUNCTION__ &lt;&lt; ") on line " &lt;&lt; __LINE__ &lt;&lt; endl; cout &lt;&lt; "# ERR: " &lt;&lt; e.what(); cout &lt;&lt; " (MySQL error code: " &lt;&lt; e.getErrorCode(); cout &lt;&lt; ", SQLState: " &lt;&lt; e.getSQLState() &lt;&lt; " )" &lt;&lt; endl; &#125; cout &lt;&lt; endl; return EXIT_SUCCESS;&#125; 结果：与数据库中结果一致，正确连接！]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>C++</tag>
        <tag>Connector/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL存储过程与函数]]></title>
    <url>%2F2018%2F07%2F22%2FMySQL%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E4%B8%8E%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[参考：深入MySQL用户自定义变量：使用详解及其使用场景案例MySQL存储过程_创建-调用-参数 简介所谓存储过程是由过程化SQL语句编写的过程，该过程经过编译和优化处理后存储在数据库服务器中，在使用时可以随时调用。函数与存储过程类似，也是用户定义的持久化存储模块，不同之处在于函数需要指定返回类型。 使用案例数据库的创建主要包括以下几个表：系的信息表 Department（Dno,Dname,Daddress）学生信息表Student（Sno, Sname, Ssex, Sage, Dno）教师信息表Teacher （Tno, Tname, Ttitle, Dno）课程信息表 Course （Cno, Cname, Cpno, Ccredit）学生选课表SC（Sno,Cno,Grade）教师授课表TC（Tno,Cno,Site） 无参存储过程1、定义一个无参数存储过程DecreaseGrade，更新所有学生成绩，将其降低5%；并调用该存储过程12345678delimiter $create procedure decreasegrade () begin update SC set Grade = Grade*0.95; end $delimiter ;call decreasegrade; 需要简单说明以下这里的一些参数因为在MySQL中默认是以; 为运行结束的界定符的，而在begin...end 块可能有多个执行语句，而每个执行语句又需要以; 结束，所以需要使用delimiter 关键字将界定符暂时改为自定义的符号，这里我习惯改为$ ，而在执行完毕以后使用delimiter ; 改回为;调用一个无参存储过程只需要使用call procedurename 的形式即可。 带参存储过程2、定义一个带输入参数存储过程IncreaseGrade，将课程号为1的所有学生成绩提升5%；要求课程号作为存储过程参数传入，并调用该存储过程12345678delimiter $create procedure increasegrade ( in ccno int) begin update SC set Grade = Grade*1.05 WHERE Cno=ccno; end $delimiter ;call increasegrade(1); 这里是带参数的存储过程，对于带参数的存储过程，分为以下三种：IN输入参数：表示调用者向过程传入值（传入值可以是字面量或变量）OUT输出参数：表示过程向调用者传出值(可以返回多个值)（传出值只能是变量）INOUT输入输出参数：既表示调用者向过程传入值，又表示过程向调用者传出值（值只能是变量） 3、定义一个带有输入和输出参数的存储过程AverageStudentGrade，计算一个学生的所有选修课程的平均成绩，要求学号作为输入参数，计算结果——该生的所有选修课平均成绩作为输出参数；调用该存储过程，并输出计算结果123456789101112delimiter $create procedure averagestudentgrade ( in paramsno VARCHAR(20), out paramgrade float) begin declare g float default 0.0;select sg.ag into g from (select Sno s, avg(grade) ag from SC group by Sno) sg where sg.s=paramsno;end $delimiter ;call averagestudentgrade(20091000863, @g); 删除存储过程4、删除存储过程IncreaseGrade、DecreaseGrades 12DROP PROCEDURE DECREASEGRADE;DROP PROCEDURE INCREASEGRADE;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>存储过程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库备份与还原--mysqldump]]></title>
    <url>%2F2018%2F07%2F22%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E4%B8%8E%E8%BF%98%E5%8E%9F--mysqldump%2F</url>
    <content type="text"><![CDATA[参考：mysql备份与还原 数据库备份格式：mysqldump -h主机名 -P端口 -u用户名 -p密码 –database 数据库名 &gt; 文件名.sql主机以及端口名可以省略不写比如我root下有数据库University 可以使用如下命令备份1mysqldump -u root -p University&gt;University.sql 接下来会提示输入数据库root密码，最后会将数据库备份到当前目录名为University.sql的文件中，或者可以自定义路径(路径存在)1mysqldump -u root -p University&gt;～/Documents/mysql_backup/University.sql 表备份格式：mysqldump -h主机名 -P端口 -u用户名 -p密码 –database 数据库名 表名 &gt; 文件名.sql 1mysqldump -u root -p University Student Course&gt;sc.sql 还原1mysql -u root -p [dbname] &lt; backup.sql 另外可以使用source命令来导入数据库常用source命令mysql&gt;source d:\test.sql，后面的参数为脚本文件。source ~/Documents/University.sql]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库备份还原</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）解决Ubuntu无法从外部应用启动Chrome打开链接的问题]]></title>
    <url>%2F2018%2F07%2F22%2F%EF%BC%88%E8%BD%AC%EF%BC%89%E8%A7%A3%E5%86%B3Ubuntu%E6%97%A0%E6%B3%95%E4%BB%8E%E5%A4%96%E9%83%A8%E5%BA%94%E7%94%A8%E5%90%AF%E5%8A%A8Chrome%E6%89%93%E5%BC%80%E9%93%BE%E6%8E%A5%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[遇到的问题：从外部应用（如WPS Presentation）打开连接后，chrome只显示一个空的标签，不会自动跳转 平台：Ubuntu 16.04 Desktop解决办法：https://askubuntu.com/questions/689449/external-links-are-opened-as-blank-tabs-in-new-browser-window-in-chrome The issue is with google-chrome.desktop, and it is missing the %U argument . Open file: $HOME/.local/share/applications/google-chrome.desktop Find the line: Exec=/opt/google/chrome/chrome Add a space and %U: Exec=/opt/google/chrome/chrome %U Then save the file.翻译一下： 出现这个问题跟 google-chrome.desktop 有关，它缺少一个参数 %U。 打开文件：$HOME/.local/share/applications/google-chrome.desktop 找到下面这行: Exec=/opt/google/chrome/chrome 在末尾添加一个空格和%U: Exec=/opt/google/chrome/chrome %U 然后保存文件即可。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java引用传递]]></title>
    <url>%2F2018%2F07%2F16%2FJava%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92%2F</url>
    <content type="text"><![CDATA[引用传递的实质就是不同栈内存空间可以指向同一块堆内存空间，并对其内容进行修改，以下通过３个代码案例详细介绍 案例一123456789101112131415161718192021222324class Message&#123; private int anInt = 10; public Message(int anInt)&#123; this.anInt = anInt; &#125; public void setAnInt(int anInt) &#123; this.anInt = anInt; &#125; public int getAnInt() &#123; return anInt; &#125;&#125;public class ReferenceDemo &#123; public static void main(String[] args)&#123; Message message = new Message(30); fun(message); System.out.println(message.getAnInt()); ｝ public static void fun(Message temp)&#123; temp.setAnInt(100); &#125;&#125; 这里需要注意的就是对于程序的执行过程都是从右向左执行，对于内存进行简单的分析可以知道， 案例二12345678910111213public class ReferenceDemo &#123; public static void main(String[] args)&#123; String string = "hello"; fun(string); System.out.println(string); &#125; public static void fun(String temp)&#123; temp = "world"; &#125;&#125;输出：hello 案例三12345678910111213141516171819202122232425262728class Message&#123; private String strInfo = "Hello"; public Message(String strInfo)&#123; this.strInfo = strInfo; &#125; public void setStrInfo(String strInfo) &#123; this.strInfo = strInfo; &#125; public String getStrInfo() &#123; return strInfo; &#125;&#125;public class ReferenceDemo &#123; public static void main(String[] args)&#123; Message message = new Message("NiHao"); fun(message); System.out.println(message.getStrInfo()); &#125; public static void fun(Message temp)&#123; temp.setStrInfo("World"); &#125;&#125;输出：World]]></content>
      <categories>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu+SVN+无法连接服务器]]></title>
    <url>%2F2018%2F07%2F16%2FUbuntu%2BSVN%2B%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[项目要求要用svn，系统在这之前装上了Ubuntu想着使用svn，最开始很开心的知道Ubuntu软件源有内置的svn，好，从命令行安装成功（这部分自行网上教程）接着开始按照教程把项目迁到本地 1svn co [项目地址] 连不上服务器？查询了服务器是对的，开始各种搜索，问别人，没有解决是不是服务器没开？主机不存在？而且在Windows上输入是可以打开的，可以进入svn项目中Ubuntu跟Windows不一样？最后询问了老板服务器IP，ping了一下是可以ping到的那就是网址错误？原来是要把服务器名字改为IP地址，然后才可以 1svn co https://192.168.0.139/svn/devs 这样才可以，现在还不清楚为什么Windows可以直接用名字，而Ubuntu只能用IP地址]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark集群搭建--节点DataNode网页无显示]]></title>
    <url>%2F2018%2F07%2F16%2FSpark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA--%E8%8A%82%E7%82%B9DataNode%E7%BD%91%E9%A1%B5%E6%97%A0%E6%98%BE%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[按照网上教程搭建好Hadoop以及Spark集群后，最开始都可以正常显示，但是有一台节点后面系统重装，再搭建过程出现问题，localhost:50070始终无法看到DataNode信息，找了很多教程试了几种方法没有解决，Spark集群的显示是正常的最后发现在Hadoop format以及启动关闭过程中/usr/local/hadoop/dfs 目录下生成data文件，网上说里面的version 文件中信息需要修改，改了也没有成功，最后想到反正都是自动生成的，可能在格式化过程中出现了问题，原有版本和新版本出现冲突，最后索性直接把这个目录下的文件删除，又把/usr/local/hadoop/tmp 重生成的文件都删除接着启动，还是不行，最后想到是不是不只得删master 中的还得删除节点的，于是把所有节点的对应文件都删除，再次启动完美运行]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Spark集群</tag>
        <tag>DataNode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向HDFS提交数据并读取测试WordCount案例]]></title>
    <url>%2F2018%2F07%2F16%2F%E5%90%91HDFS%E6%8F%90%E4%BA%A4%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%AF%BB%E5%8F%96%E6%B5%8B%E8%AF%95WordCount%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[要使用hdfs需要切换到Hadoop安装目录下，然后使用命令 1bin/hdfs dfs -ls / 可以查看对应节点hdfs文件系统下的目录结构接着将测试数据放到hdfs上，这里是将数据放到/testdata目录下 1bin/hdfs dfs -put /word.txt /testdata 注意这里的word.txt是自己在namenode根目录创建的文件，内容如下：查看是否上传: 1bin/hdfs dfs -ls /testdata 可以看到对应的文件已经上传到hdfs文件系统中接着测试在spark-shell中直接编写wordcount程序，并从hdfs读取数据，最后返回结果进入spark安装目录，并启动spark-shell首先指定要读取的文件路径：这里需要写hdfs的路径： 1val file = sc.textFile("hdfs://your.master.ip:9000/testdata/word.txt") 接着进行计算： 1val rdd = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word,1)).reduceByKey(_+_) 使用collect将计算的数据存储，并返回： 1rdd.collect() 最后打印显示count的结果： 1rdd.foreach(println) OK，到此简单的测试了如何将本地数据上传到hdfs文件系统，并使用spark-shell实现WordCount案例，并将计算后的数据打印输出。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>HDFS</tag>
        <tag>WOrdCount</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)通过漫画轻松掌握HDFS工作原理]]></title>
    <url>%2F2018%2F07%2F16%2F%E9%80%9A%E8%BF%87%E6%BC%AB%E7%94%BB%E8%BD%BB%E6%9D%BE%E6%8E%8C%E6%8F%A1HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[(转)通过漫画轻松掌握HDFS工作原理 原文]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSQL读取json文件简单实现（一）]]></title>
    <url>%2F2018%2F07%2F16%2FSparkSQL%E8%AF%BB%E5%8F%96json%E6%96%87%E4%BB%B6%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[运行前提：搭建Hadoop + Spark集群，安装jdk1.8及以上版本，Scala2.11，maven3 1、新建maven项目这里需要注意的是我们为了方便，直接勾选Create from archetype，然后从下面列表中选择scala archetype simple 2、填写GAV这里按照个人设置，说白了GroupID就是公司的域名倒过来写artifactId可以是你项目的名称version就是项目的版本号这些只是一个项目的识别，后面会体现在项目的pom.xml文件中 3、设置maven相关路径这里需要注意的就是填写setings.xml需要是你maven的设置文件的路径，第一次我按照默认的路径生成项目在运行时候报错，后面我找到了设置文件在maven的安装目录下比如我的是：/opt/apache-maven-3.5.3/conf/settings.xml其他设置按照个人喜好 4、填写项目名称接着点击finish即可 5、本地测试将原有的Scala文件删掉，重写本地的测试代码123456789101112131415161718192021222324import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.SQLContext/** * SQLContext的使用 * */object SQLContextApp &#123; def main(args: Array[String]): Unit = &#123; val path = args(0) //1、创建相应的context val sparkConf = new SparkConf() sparkConf.setAppName("SQLContextApp").setMaster("local[*]") val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc) //2、进行相关的处理 val people = sqlContext.read.format("json").load(path) people.printSchema() people.show() //3、关闭资源 sc.stop() &#125;&#125; 这里需要注意的是：我们的args（0） 在本地测试时候是读取的本地的文件路径，可以在项目中进行配置找到项目的edit configuration按照下图进行设置：这里修改的是Program arguments 修改路径为：file:///usr/local/spark/examples/src/main/resources/people.json当然这里是我的json文件所在的路径，我是直接拿Spark中自带的json文件进行测试，你也可以自己按照一定的格式读取另外，可能有的坑就是在pom.xml 文件中关于Scala以及SparkSQL的依赖需要我们手动添加，因为项目最初建立的时候默认的Scala的版本比较低，需要修改为你的安装版本，比如我的是如下配置：先指定properties 1234&lt;properties&gt; &lt;scala.version&gt;2.11.12&lt;/scala.version&gt; &lt;spark.version&gt;2.1.0&lt;/spark.version&gt; &lt;/properties&gt; 然后添加依赖，如果是初次运行maven项目的话可能要下载很多依赖，要等待下完12345678910111213&lt;!---scala dependency --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!---SparkSQL dependency--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; 到这里基本的环境配置就完成了maven会自动进行编译，如下图：看到BUILD SUCCESS 还是很开心的 6、运行结果截图： 总结虽然实现了最终的结果，对于SparkSQL进行了简单的测试输出，但是由于是第一次实现，还是有很多不足的地方，比如代码中的函数已经过时，新的代码使用规范将会在后面进行测试，另外后面会写关于如何在集群上进行测试的教程。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSQL读取json文件简单实现（二）]]></title>
    <url>%2F2018%2F07%2F16%2FSparkSQL%E8%AF%BB%E5%8F%96json%E6%96%87%E4%BB%B6%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[继上一篇文章中简单介绍了如何在本地调试SparkSQL，详细地址，接下来我们测试如何将该功能在集群上实现，毕竟我们生产生活中数据量是很大，需要借助集群来进行相关的功能 编译首先，我们需要将代码中设置的测试参数注释掉，这里推荐在本地测试的时候设置AppName以及Master ，而如果是想要在集群上运行，我们将对应的地方注释 1234//在测试或者生产中 AppName以及Master我们是通过脚本进行指定的（推荐）//sparkConf.setAppName("SQLContextApp").setMaster("local[*]")val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc) 接下来我们进入项目所在的路径 1cd /home/hadoop/Documents/IdeaProjects/XXXProject 然后输入 1mvn clean package -DskipTests 这个过程可能会下载一些Jar包，耐心等待即可 OK，没什么问题可以看到其中有一句 1Building jar: /home/hadoop/Documents/IdeaProjects/XXXProject/target/sql-1.0.jar 那么编译的jar包就在这个路径，那么我们接下来要做的就是将这个jar包传到服务器进行测试，因为我这里是在本机搭建的服务器，所以就直接使用本机进行测试，如果你连接的是远程的服务器，你可以使用scp 命令进行上传比如我现在要传到服务器根目录的/lib 目录下首先我们在home 目录创建这个文件夹mkdir lib然后进入jar 包所在目录上传scp sql-1.0.jar hadoop@slave02:/home/hadoop/lib 提交运行这里使用spark-submit 提交运行具体的参数命令：12345spark-submit \--class path.to.your.main.class \--master local[2] \/home/hadoop/lib/sql-1.0.jar \/usr/local/spark/examples/src/main/resources/people.json 这里我运行出了问题查看日志，发现无法连接到主机，应该是集群的问题，我想起来我这台电脑之前配置的集群是一个子节点，不是主节点，于是通过ssh 登录到远程服务器，并将本地的jar包上传到服务器的/data/driverLib 目录下然后在服务器上重新运行，又出现了以下错误：这时候仔细看输出应该是读取的输入路径不正确，原来指定的参数默认在服务器上是到**HDFS** 文件系统中读取文件，OK，简单，我直接把文件上传到HDFS 上应该就可以了 1hadoop fs -put /usr/local/spark-2.2.1/examples/src/main/resources/people.json hdfs://your/hdfs/path 这里涉及到HDFS 相关命令的使用所有的问题应该都解决了，好，开始运行 完美 输出！！！！！ 参考：https://spark.apache.org/docs/2.0.0/submitting-applications.html]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSQL读取json文件简单实现（三）]]></title>
    <url>%2F2018%2F07%2F16%2FSparkSQL%E8%AF%BB%E5%8F%96json%E6%96%87%E4%BB%B6%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[继上面实践了一些简单的读取文件，简单操作了SparkSQL的入口方法，但是要知道之前的方法都是已经过时了的方法，是Spark1.x中使用的，而最新的2.x版本使用SparkSession作为SparkSQL的入口函数接下来我们通过简单的代码来在集群操作测试 定义基本的使用格式： 12345val spark = SparkSession .builder() .appName("Spark SQL basic example") .config("spark.some.config.option", "some-value") .getOrCreate() 测试了解了基本的定义格式，我们接下来还是以之前的json 文件进行测试：在项目中新建一个SparkSessionApp，注意是新建的scala class 选择object 类型代码如下： 123456789101112131415/** * SparkSession的使用 */object SparkSessionApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .appName("SparkSessionApp") .getOrCreate() val people = spark.read.format("json").load("file:///usr/local/spark/examples/src/main/resources/people.json") people.show() spark.stop() &#125;&#125; 具体的config 参数后面详细介绍，我们先按照官网案例进行简单的输出，运行：出错了，仔细查看错误信息 1org.apache.spark.SparkException: A master URL must be set in your configuration 应该是必须指定master直接在SparkSession 后加上.master(local[2])再次运行：成功输出！ 参考：http://spark.apache.org/docs/2.1.0/sql-programming-guide.html#starting-point-sparksession]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
        <tag>SparkSession</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DataFrame数据操作以及与RDD互相操作案例]]></title>
    <url>%2F2018%2F07%2F16%2FDataFrame%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E4%B8%8ERDD%E4%BA%92%E7%9B%B8%E6%93%8D%E4%BD%9C%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Dataframe基本操作主要对于DataFrame学习的一些简单操作，代码如下： 1234567891011121314151617181920212223242526272829303132333435import org.apache.spark.sql.SparkSession/** * DataFrame 基本操作 */object DataFrameApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master("local[2]") .appName("DataFrameApp") .getOrCreate() val peopleDF = spark.read.format("json").load("file:///usr/local/spark/examples/src/main/resources/people.json") //输出dataframe对应的schema peopleDF.printSchema() //输出前20条信息（默认值），可以指定 peopleDF.show() //查询某列所有的数据 :select name from table peopleDF.select("name").show() //查询某几列的数据并做相应计算：select name, age + 10 as new age from table peopleDF.select(peopleDF.col("name"), (peopleDF.col("age") + 10).as("new age")).show() //查询年龄大于19的 ： select * from table where age &gt; 19 peopleDF.filter(peopleDF.col("age") &gt; 19).show() //根据某一列进行分组，然后再进行聚合操作 select age, count(1) from table group by age peopleDF.groupBy("age").count().show() spark.stop() &#125;&#125; 进阶操作DataFrame与RDD互相操作—反射方式1234567891011121314151617181920212223242526272829303132import org.apache.spark.sql.SparkSession/** * DataFrame与RDD互相操作实现---反射实现 */object DataFrameRDDApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .appName("DataFrameRDDApp") .master("local[2]") .getOrCreate() //RDD =&gt; DataFrame val rdd = spark.sparkContext.textFile("/home/hadoop/infos.txt") //注意：需要导入隐式转换 import spark.implicits._ val infoDF = rdd.map(_.split(",")).map(line =&gt; Info(line(0).toInt, line(1), line(2).toInt)).toDF infoDF.show() //进行简单查询操作 infoDF.filter(infoDF.col("age") &gt; 30).show() //通过转化为info表然后调用sqlAPI进行操作 infoDF.createOrReplaceTempView("info") spark.sql("select * from info where age &gt; 30").show() spark.stop() &#125; case class Info(id: Int, name: String, age: Int)&#125; DataFrame与RDD互相操作—编程方式123456789101112131415161718def program(spark: SparkSession): Unit = &#123; /** * RDD =&gt; DataFrame 编程方式 */ // Create an RDD val rdd = spark.sparkContext.textFile("/home/hadoop/infos.txt") val infoRDD = rdd.map(_.split(",")).map(line =&gt; Row(line(0).toInt, line(1), line(2).toInt)) val structType = StructType(Array(StructField("id", IntegerType, true), StructField("name", StringType, true), StructField("age", IntegerType, true))) val infoDF = spark.createDataFrame(infoRDD, structType) infoDF.printSchema() infoDF.show() &#125; 参考：http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
        <tag>DataFrame</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现俩个整数变量的交换]]></title>
    <url>%2F2018%2F07%2F16%2F%E5%AE%9E%E7%8E%B0%E4%BF%A9%E4%B8%AA%E6%95%B4%E6%95%B0%E5%8F%98%E9%87%8F%E7%9A%84%E4%BA%A4%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[临时变量通过临时变量实现交换： 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; int x = 10; int y = 5; int temp; temp = x; x = y; y = temp; System.out.println("x = " + x + " y = " + y); &#125;&#125;x = 5 y = 10Process finished with exit code 0 两数相加1234567891011121314public class Test &#123; public static void main(String[] args) &#123; int x = 10; int y = 5; x = x + y; y = x - y; x = x - y; System.out.println("x = " + x + " y = " + y); &#125;&#125;x = 5 y = 10Process finished with exit code 0 运算符1234567891011121314public class Test &#123; public static void main(String[] args) &#123; int x = 10; int y = 5; x = x ^ y; y = x ^ y; x = x ^ y; System.out.println("x = " + x + " y = " + y); &#125;&#125;x = 5 y = 10Process finished with exit code 0]]></content>
      <categories>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
        <tag>变量交换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Java中String、StringBuffer、StringBuilder]]></title>
    <url>%2F2018%2F07%2F16%2F%E6%B5%85%E8%B0%88Java%E4%B8%ADString%E3%80%81StringBuffer%E3%80%81StringBuilder%2F</url>
    <content type="text"><![CDATA[参考：https://www.programmergate.com/stringbuilder-vs-stringbuffer/https://blog.csdn.net/rmn190/article/details/1492013https://droidyue.com/blog/2014/12/21/string-literal-pool-in-java/ String 字符串常量 StringBuffer 字符串变量（线程安全）StringBuilder 字符串变量（非线程安全） StringStrings are constant; their values cannot be changed after they are created.这是JavaAPI中对于String的定义，就是说String是常量，一旦定义就无法改变。说到常量就不得不提到Java中的字符串常量池，通过String的俩种创建方法来进行了解 String俩种创建方式区别Java中字符串对象创建有两种形式，一种为字面量形式，如String str = “abc”;，另一种就是使用new这种标准的构造对象的方法，如String str = new String(“abc”);当Java中出现第一种字面量形式创建字符串时，JVM首先会在常量池中进行查找是否存在相同字符串对象内容的引用，如果存在，那么直接将新创建的对象指向该引用地址，如果不存在则创建新的引用对象，并把该引用对象放入常量池 1234567public class Test &#123; public static void main(String[] args) &#123; String str1 = "abc"; String str2 = "abc"; System.out.println(str1 == str2); // true &#125;&#125; 通过以上代码可以看到结果为true 也就是说str1 str2 指向的是相同的地址。现在将代码改变： 12345678public class Test &#123; public static void main(String[] args) &#123; String str1 = "abc"; str1 = "xyz"; String str2 = "abc"; System.out.println(str1 == str2); // false &#125;&#125; 当JVM看到&quot;xyz&quot;，在字符串常量池创建新的String对象存储它，再把新建的String对象的引用返回给str1，所以这时候str1 指向的是xyz ，而abc 还是存在于字符串常量池中，其值并不能被改变，所以返回false接下来继续将代码改为以下： 12345678910public class Test &#123; public static void main(String[] args) &#123; String str1 = "abc"; str1 = "xyz"; String str2 = "abc"; System.out.println(str1 == str2); // false String str3 = new String("abc"); System.out.println(str2 == str3); // false &#125;&#125; 通过new 来创建一个新的String对象str3 ，这个过程是首先，JVM在常量池中找是否存在abc ，如果找到，不做任何事情，如果没找到，在常量池中创建该对象；接着由于有new 关键字，会在栈内存空间（不是字符串常量池）中创建str3 对象，并指向堆内存空间（不是字符串常量池）中的abc ，接下来进行str2 == str3 比较的时候肯定不是引用的同一个对象，所以返回false所以说使用new创建String对象过程中是产生了俩个对象另外Java中还提供了如何将new创建的对象手动放入常量池，看以下代码： 123456789101112public class Test &#123; public static void main(String[] args) &#123; String str1 = "abc"; str1 = "xyz"; String str2 = "abc"; System.out.println(str1 == str2); // false String str3 = new String("abc"); System.out.println(str2 == str3); // false String str4 = str3.intern(); System.out.println(str2 == str4); // true &#125;&#125; 这样结果就返回的是true StringBuilder VS StringBuffer通过上面的介绍我们简单的了解了String是不可变的，是一个常量，因此当我们使用String来构建一个动态的字符串对于内存的开销是特别大的，因为对于每个字符串我们都得分配新的内存空间所以在Java开发初就提供了StringBuffer JavaAPI对于StringBuffer 的说明是A thread-safe, mutable sequence of characters.这个可变的类来创建动态的字符数组，而这个只需要分配一次内存，这与String相比是节省了大量的内存空间，另外，由于其是线程安全的，所以其所有的public 方法都是synchronized ，而这样的设计就会导致，当开发人员是在单线程的环境中时，就会增加额外的开销，而且大多数对于StringBuffer 的使用基本上都是在单线程的环境中，所以开发人员根本不需要对于synchronized 的额外开销，所以在jdk1.5之后提供一个新的类StringBuilder ，这个类不是线程安全的也不是同步的，但是其使用更加方便而且在单线程环境中更加快速 性能测试下面通过代码进行测试当创建动态的字符数组时String、StringBuffer、StringBuilder 三者需要的时间对比，测试代码如下： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; new Test().constructDynamicString(); &#125; public void constructDynamicString() &#123; long startTime = System.currentTimeMillis(); String s = "test"; //StringBuffer s = new StringBuffer("test"); //StringBuilder s = new StringBuilder("test"); for(int i=0; i&lt;100000; i++) &#123; s += "concat"; //s.append("concat"); &#125; long endTime = System.currentTimeMillis(); System.out.println("Concat time ====== " + (endTime - startTime) + "ms"); &#125;&#125; 结果如下： 123String创建时间： 43144msStringBuffer创建时间： 14msStringBuilder创建时间： 14ms 第一次测试结果都是14ms，应该是数字较小，循环1000000次，统计结果 12StringBuffer： Concat time ====== 80msStringBuilder： Concat time ====== 43ms 通过以上测试可以看出，在创建动态字符串过程中，使用String需要的时间最久，而使用StringBuilder时间最短，而且创建的次数越多这个性能越明显。 总结通过以上的分析，那么在实际使用中我们怎么选择呢？ 1、如果创建静态的字符串而且在程序中是不变的，那么选择String2、如果是创建动态字符串，并且是在多线程中使用，那么用StringBuffer3、其他的都使用StringBuilder，并且大多数情况下都是使用StringBuilder]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>String</tag>
        <tag>StringBuffer. StringBuilder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL+IDEA实现JDBC连接]]></title>
    <url>%2F2018%2F07%2F16%2FMySQL%2BIDEA%E5%AE%9E%E7%8E%B0JDBC%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[参考：MySQL JDBC Driver 5.1.33 - Time Zone IssueMySQL Connectors Java下载及使用win10系统，mysql-installer-community-5.7.21.msi安装 环境WIndows10+MySQL8.0+IDEA2017.3Enterprise MySQL安装这部分详细不多说，详细参考以上所列文章，需要注意的地方就是，在选择安装类型步骤时，如果不想安装其他组件，可以选择Custom 安装，这里安装其实可以勾选上Connector/J 也就是MySQL通过Java连接数据库的驱动，方便后面添加这个库；另外就是一定要记住自己安装时的密码！！！所有步骤安装完成以后，可以看到如下目录这时候点击MySQL 8.0 Command Line Client则会进入命令行模式这时候输入我们之前创建的密码接下来就可以输入简单的SQL命令来使用数据库了，比如查询所有的数据库 创建IDEA项目第一步在IDEA中新建工程选择Java类型，设置ProjectSDK，接着输入工程名称和存放位置，即可生成一个Java项目第二步 添加依赖jar包点击File-&gt;Project Structure-&gt;Libraries ，再点击绿色的加号，在之前MySQL安装路径下可以找到mysql-connector-java-8.0.11确定即可，这里如果默认安装路径的话就是在C:\Program Files (x86)\MySQL\Connector J 8.0 下另外，如果之前在安装MySQL时候没有选择Connector\J 也可以到官网下载然后直接导入，结果是一样的。成功导入以后就可以在工程的External Libraries 看到多出来的 通过JDBC连接数据库在之前创建的项目src 文件夹下新建类Main 在类中实现连接数据库，代码如下： 1234567891011121314151617181920212223/** * 使用JDBC连接University数据库 */ public static Connection getConnection() throws Exception&#123; Connection conn = null; // 创建一个connection try &#123; Class.forName("com.mysql.cj.jdbc.Driver"); String url = "jdbc:mysql://localhost:3306/University?" + "user=YaboSun&amp;password=root" + "&amp;useUnicode=true&amp;characterEncoding=UTF8" + "&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC"; conn = DriverManager.getConnection(url); &#125; catch (ClassNotFoundException e) &#123; System.out.println("JDBC Driver not found"); e.printStackTrace(); &#125; catch (SQLException e) &#123; System.out.println("SQL 执行错误"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return conn; &#125; 功能测试已经建立了一个连接，接下来就是测试以下是否连接成功，并实现数据库的增、删、查、改等简单操作在主函数中实现对应功能进行测试，代码如下： 12345678910111213141516171819202122232425262728293031323334353637public static void main(String[] args) throws Exception&#123; String sql; // 用于后面对应不同的sql语句 Connection connection = getConnection(); // 获取一个connection /** * 创建一个statement * Statement里面带有很多方法，比如executeUpdate可以实现插入，更新和删除 * executeQuery可以实现查询 */ Statement statement = connection.createStatement(); sql = "create table Test(Tno char(11), Tname char(20), Tage int)"; int updateResult ; // 用于获取是否创建成功 如果不成功结果为-1 try &#123; updateResult = statement.executeUpdate(sql); if (updateResult != -1) &#123; sql = "INSERT INTO Test VALUES ('20131002318', '本科', 19)"; updateResult = statement.executeUpdate(sql); sql = "INSERT INTO Test VALUES ('1201721379', '硕士', 23)"; updateResult = statement.executeUpdate(sql); sql = "SELECT * FROM Test"; ResultSet resultSet = statement.executeQuery(sql); // executeQuery会返回结果的集合，否则返回空值 while (resultSet.next()) &#123; String Tno = resultSet.getString("Tno"); String Tname = resultSet.getString("Tname"); int Tage = resultSet.getInt("Tage"); System.out.println(Tno + " " + Tname + " " + Tage); &#125; &#125; &#125; catch (SQLException e) &#123; System.out.println("Test表已经存在"); e.printStackTrace(); &#125; sql = "DROP TABLE Test"; updateResult = statement.executeUpdate(sql); System.out.println("Test表已经删除"); connection.close(); &#125; 得到正确结果 这只是简单的进行一些测试，代码的优化还需要很大改进，如有不正确之处，望能指出。]]></content>
      <categories>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String实例化的两种方法及其区别]]></title>
    <url>%2F2018%2F04%2F13%2FString%E5%AE%9E%E4%BE%8B%E5%8C%96%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%E5%8F%8A%E5%85%B6%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[对于Java中String对象的实例化来说，总共有两种实例化方法 直接赋值1String strA = "Hello World"; 利用构造方法new1String strB = new String("Hello World"); String对象的实例化比较简单，但是我们这里主要是深入理解其底层的设计以及在内存中的区别，接下来说一下，通过这两种方法实例化的对象有什么不同之处 区别首先，通过两种不同的方式创建的strA 以及 strB ，测试是否相等1System.out.println(strA == strB); 结果输出为false原因是对于操作符&quot;==&quot; 来说，进行比较的是数值 也就是说，这时候比较的是在堆内存空间中存放的字符串的地址，显然是不相等的，那么如果我们想要比较的是String对象的内容，需要使用public boolean equals(str) 1System.out.println(strB.equals(strA)); 此时输出的结果为true继续分析对于第一种方法的创建来说，是直接在堆内存空间中开辟一块内存，存放Hello World， 实际上字符串常量相当于是String的匿名对象，所以在栈中有名为strA 的对象指向这块堆内存而对于第二种方法，内存分析情况为，先是在堆内存空间中开辟内存存放Hello World ，接下来在new 的过程中又在不同的堆内存地址开辟空间并存放Hello World ，而在栈内存空间中的strB 指向的是后面开辟的堆内存空间，这样导致开辟了两块内存空间，那么第一块内存空间就成了我们所谓的垃圾 接下来，假设我们又定义如下对象： 1String strC = "Hello World"; 那么对于 1System.out.println(strA == strC); 结果是true这是因为，对于直接赋值，这里涉及到共享设计模式 的概念，就是说在JVM的底层设计中有对象池，当创建某个对象时，就会将这个对象的匿名对象入池保存，而后如果有相同的方式创建对象，并且所对应的匿名对象是相同的内容，那么就不会在堆内存中开辟新的内存空间，而是将栈内存中的对象直接指向已有的地址，所以结果为true对于第二种方式创建的对象，是不会在对象池中保存，如果想要入池，需要手工入池，使用的方法是：public String intern() 12String strD = new String("Hello World").intern();System.out.println(strD == strA); 此时输出的结果为true 总结综合以上分析，这里只是为了分析比较才有了第二种创建的方法，而在实际工作中是都采用直接赋值的方法。 附String类中需要记忆的常用方法，持续更新中。。。| 方法 | 类型 | 描述 ||—|—|—|| public String(char[] value) | 构造 | 将字符数组转换为String || public String(char[] value,int offset,int count) | 构造 | 将指定索引区间的字符数组转换为String || public char charAt(int index) | 普通 | 返回指定索引对应的字符信息 || public char[] toCharArray() | 普通 | 将字符串以字符数组的形式返回 |]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE--自增自减运算]]></title>
    <url>%2F2018%2F04%2F07%2FJavaSE--%E8%87%AA%E5%A2%9E%E8%87%AA%E5%87%8F%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Java中，对于初学者来说，很容易混淆的一个知识点就是对于运算符这部分关于自增自减的理解，自增(++)和自减(–)两种运算符，这两种运算符帮助我们在编写程序的时候简化了代码。这里需要注意的几点如下： 1234567891011121314public class NumAdd &#123; public static void main(String []args)&#123; /** * 1、这里不管执行++还是--都是需要确定的值 * 2、b = a++（--）执行顺序是先赋值再自增（自减） * 3、b = ++（--）a执行顺序是先自增（自减）再赋值 */ int a = 3; int b = a++; System.out.println(b); int c = ++b; System.out.print(c); &#125;&#125; 通过代码可以看出自增和自减运算符号放在变量的前面，如 ++a, 表示先执行运算，再生成值， 放在变量的后面，如: a++,先生成值，再执行运算]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>运算符</tag>
        <tag>自增自减</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE--简单Java类开发要求]]></title>
    <url>%2F2018%2F04%2F07%2FJavaSE--%E7%AE%80%E5%8D%95Java%E7%B1%BB%E5%BC%80%E5%8F%91%E8%A6%81%E6%B1%82%2F</url>
    <content type="text"><![CDATA[简单Java类的开发应该是所有Java开发过程中最简单也是最基础的一种开发，说白了就是不会涉及到一些循环、判断等结构的Java类开发，比如说简单的一个图书管理系统等等，通常是Java基础学习完成后第一个小的项目。对于简单开发类，开发要求如下：1、类名称必须存在有意义，比如说：Book、Empire等2、类中的所有属性必须使用private封装，封装后的属性必须提供setter和getter方法3、类之中可以提供任意多个构造方法，但是必须保留有一个无参构造方法4、类之中不允许出现任何的输出语句，所有的输出必须交给被调用处5、类之中需要提供一个取得对象完整信息的方法，暂定为getInfo，而且返回string类型 简单案例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * 定义一个有意义的类：雇佣员工类 */class Empire&#123; private int mnEmpNum; private String mstrEmpName; private String mstrJob; private double mdSalary; private double mdComm; /** * 无参构造函数 */ public Empire()&#123;&#125; /** * 有参构造函数 */ public Empire(int nNum, String strName, String strEmpJob, double dSal, double dCom)&#123; mnEmpNum = nNum; mstrEmpName = strName; mstrJob = strEmpJob; mdSalary = dSal; mdComm = dCom; &#125; /** * setter方法 * @param nEmpNum */ public void setMnEmpNum(int nEmpNum)&#123; mnEmpNum = nEmpNum; &#125; public void setMstrEmpName(String strEmpName)&#123; mstrEmpName = strEmpName; &#125; public void setMstrJob(String strJob)&#123; mstrJob = strJob; &#125; public void setMdSalary(double dSalary)&#123; mdSalary = dSalary; &#125; public void setMdComm(double dComm)&#123; mdComm = dComm; &#125; /** * getter方法 * @return */ public int getMnEmpNum() &#123; return mnEmpNum; &#125; public String getMstrEmpName() &#123; return mstrEmpName; &#125; public String getMstrJob() &#123; return mstrJob; &#125; public double getMdSalary() &#123; return mdSalary; &#125; public double getMdComm() &#123; return mdComm; &#125; public String getInfo()&#123; return "雇员编号" + mnEmpNum + "\n" + "雇员姓名" + mstrEmpName + "\n" + "雇员职位" + mstrJob + "\n" + "雇员薪水" + mdSalary + "\n" + "佣 金" + mdComm + "\n"; &#125;&#125; 在构造了一个简单的类以后，接下来要做的事就是编写测试函数（主函数） 123456public class TestDemo &#123; public static void main(String []args)&#123; Empire empire = new Empire(001,"Frank","CEO",10245.2,3010.7); System.out.println(empire.getInfo()); &#125;&#125; 程序输出结果： 12345678雇员编号1雇员姓名Frank雇员职位CEO雇员薪水10245.2佣 金3010.7Process finished with exit code 0 对于程序中的setter以及getter方法，有时候程序中并不会用到，但是需要设置，这里也能体现类的很好的封装性]]></content>
      <categories>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>简单Java类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE--一维数组创建的几种方法]]></title>
    <url>%2F2018%2F04%2F07%2FJavaSE--%E4%B8%80%E7%BB%B4%E6%95%B0%E7%BB%84%E5%88%9B%E5%BB%BA%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数组可以说是对于每种语言学习中的最基础的数据结构之一，几乎是在所有项目中都会涉及到数组的使用，接下来就详细介绍一下数组的声明、创建和初始化以及实例代码解析 声明123数据类型 数组名称[]或数据类型[] 数组名称 //首选的方法 这部分来说俩种声明方式的效果是相同的，但是推荐首选第一种方法来声明，因为第一种方法更加直观，更具有可读性，可以通过声明看出是一种引用类型，而不是普通的数据类型具体整型数组声明如下： 123声明ｉｎｔ型数组ｉｎｔ[] arrNum;int arrNum[]; 这里的命名看个人喜好，不管怎样要有自己的规范，我个人一般是加前缀arr另外需要注意的是，Java语言中声明数组时不能指定其长度，因为数组是一种引用型变量，也就是说在数组定义时实质上是定义了一个引用变量（或者说是一个指针），而此时这个引用变量还没有指向任何有效的内存，并且这时候在内存中还没有开辟任何内存空间，所以不能指定长度.错误定义 12//int[5] arrNum;//int arrNum[5]; 那么这个时候学过C++的可能会问，为什么在C++中这样定义数组是合法的呢？而且是这样定义以后就可以使用，Java却需要创建、初始化以后才可以使用？因为C和C++中的数组就是内存块，而Java中出于安全性考虑，定义的数组是对象，数组保存的仅仅是对象的引用，而不是对象本身，Java中对象是在堆中的，因此数组无论保存原始类型还是其他对象类型，数组对象本身是在堆中的。 创建Java使用new关键字创建数组，如果数组创建没有初始化，必须指定数组的大小 123arrNum = new int[5]；//正确arrNum = new int[]&#123;0,1,2,3,4&#125;；//正确//arrNum = new int[];//错误，未初始化也未指定数组的大小 案例一： 123456789101112public class ArrayDemo &#123; public static void main(String []args)&#123; int [] arrNum; int [] arrNum1; int [] arrNum2 = new int[5]; arrNum = new int[5]; arrNum1 = new int[]&#123;1,2,3,4,5&#125;; System.out.println(arrNum[0]); System.out.println(arrNum1[0]); System.out.println(arrNum2[0]); &#125;&#125; 可以看到以上这三种方法创建数组都是正确的 初始化其实在上面的案例中已经涉及到了数组的初始化，数组的初始化分为静态初始化、动态初始化以及默认初始化动态初始化就是在创建过程中只是声明数组的大小，而由系统为数组分配值 123int [] arrNum;arrNum = new int[5];int [] arrNum = new int[5];//简化方式 静态初始化 就是由程序员显式的指定每个数组元素的值 123int [] arrNum1;arrNum1 = new int[]&#123;1,2,3,4,5&#125;;int[] arrNum1 = &#123;1,2,3,4,5&#125;;//简化方式 这里对于静态初始化的方式，推荐使用第一种方式，不推荐简化方式。 堆栈分析以上数组创建的方式，其实归根到底总共是俩种方式：方式一：12int[] arrNum;arrNum = new int[5]; 对于这种方式，实质上第一句是创建了一个引用对象，可以认为是在栈内存中开辟了一个对象，而为这个对象创建了一个空指针，这时候如果没有第二句，那么编译器就会报错空指向异常，第二句的作用就是在堆内存中开辟出大小为５的内存空间方式二： 1int[] arrNum = new int[5]; 实际上这种方法就是省去了第一步，也可以说是将对象在堆栈中开辟的过程合为了一个过程 引用传递既然前面提到了数组实际上是一种引用类型，那么数组是一定可以发生引用传递的，什么叫引用传递？实质就是同一块堆内存空间能够被不同的栈空间所访问，那么数组的引用传递是怎么实现？案例二： 12345678910111213141516171819public class ArrayDemo &#123; public static void main(String[] args) &#123; int[] arrNum; arrNum = new int[]&#123;1, 2, 3, 4, 5&#125;; int[] temp = arrNum; temp[0] = 10; for (int i = 0;i &lt; arrNum.length; i++)&#123; System.out.println(arrNum[i]); &#125; &#125;&#125;输出：102345Process finished with exit code 0 这个时候对于语句int[] temp = arrNum; 执行的就是将temp数组在栈内存空间中开开辟的地址指向和data数组同样的堆内存地址，而这个时候改变temp[0]的值实质上同时改变了arrNum[0]的值。 总结以上就是详细的学习了数组的一些相关操作，但是这只是为了掌握数据的底层一些设计，在实际工作中，不管动态还是静态的方法一般都不会用到，因为数组有一个最大的缺点就是数组的长度不能改变，实际过程中一般都是通过传值或者别的方式进行动态的生成数组，而不是提前指定数组的大小。 Java创建数组的几种方式数组在C++和java中的区别]]></content>
      <categories>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
        <tag>数组定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GeoSpark学习--初识GeoSpark]]></title>
    <url>%2F2018%2F04%2F02%2FGeoSpark%E5%AD%A6%E4%B9%A0--%E5%88%9D%E8%AF%86GeoSpark%2F</url>
    <content type="text"><![CDATA[背景 传统的不足：数据存储方面：1、现有的数据存储主要是多依赖关系型数据库，比如Oracle等，但是关系型数据库在海量数据管理、高并发读写以及扩展性方面有很大的局限2、传统的空间数据存储方式不但难以扩展，而且随着数据的激增读写性能存在极大瓶颈3、传统的分布式文件系统虽然可以存放在不同的节点上，但这种分布式文件系统所支持的扩展性有限数据分析方面：大数据时代的数据是处处连接的，每个连接都是一个传感器，这些传感器无时无刻都在进行数据的采集，所以说数据的变化很快，在这样的背景下，迫切需要GIS能够做到低延迟的分析处理工作 技术发展 1、存储去结构化非关系型数据库比如说HBase、Redis、MongoDB、InfoGrid，这些数据库不需要预先定义模式，并且可以在系统运行的时候动态增加或删除节点，避免了停机维护，提高了拓展性和可靠性另外这些非关系型数据库没有共享架构，数据一般是存储在本地服务器上，可以直接读取数据，提高了数据的读取性能。2、计算内存化对于Hadoop来说主要进行离线数据的计算，应对低延迟的应用场景比较困难，另外Hadoop使用MapReduce模型，该模型将复杂模型使用简单的映射规约，对于复杂的算法逻辑支持不充分，并且数据存储在硬盘上，很容易收IO瓶颈的影响，所以对于处理GIS数据乏力。而新的技术Spark启用了内存分布式数据集，支持更多的范式，而且配有一个数据处理模型，所以在处理GIS数据中性能更好。3、分析去模型化传统的GIS空间数据分析需要先建立分析模型，比如说考虑影响因子，权重，最后综合各影响因素进行建模，然而大数据环境下的数据是时刻在变化的，这样就无法满足，所以去模型化是最终的发展方向 GeoSpark 这里主要介绍GeoSpark以及我调试运行过程中遇到的问题GeoSpark是在Apache Spark第三方项目中的一个子项目，也是一个用于处理大规模空间数据的集群计算系统，目前最新版本更新到v1.1.0（2018年3月13日）。GeoSpark继承自Apache Spark，并拥有一系列具有创造性的空间弹性分布式数据集（SRDDs），借助机器这些数据集可以有效的加载，处理并分析大规模的空间数据。GeoSpark为Apache开发人员提供了API使得他们能够利用SRDDs方便的开发空间分析程序，这些程序为地理空间查询提供了有力的支持。 主要功能GeoSparkSQL最新版本的GeoSpark主要是包含了新的SQL功能，新增了四叉树和R树的索引解析并修复了一些bug。这个版本最主要的就是包含了一个完整的GeoSparkSQL版本，GeoSparkSQL完全支持Apache Spark SQL。SRDDs支持特殊的SRDDs，包括PointRDD,RectangleRDD,PolygonRDD以及LineStringRDD。空间分割支持的空间分割技术有四叉树、KDB树、R树、沃罗诺伊图（Voronoi diagram）、均匀网络（Uniform grids）空间索引支持四叉树、R树、以及空间K近邻查询 调试之路好，开搞，以下内容较多，纯属个人踩坑记录，想直接运行的跳到文章末尾即可1、搭建环境 系统 IDE Spark Scala Jdk Maven Sbt Ubuntu16.04 IntelliJ IDEA2017.3 2.2.1 2.11.12 1.8.0_151 3.5.3 1.1.1 基本的环境都搭建好了，没遇到什么问题，具体安装教程网上都有，就不多说，不过在我安装过程中也遇到了坑，这里就不一一说明，有遇到问题的可以问我2、Git Clone项目地址 这是什么鬼？怎么迁项目还出了问题，应该是权限的问题 稍微等一会儿就Clone好了，这时候在home可以看到 项目的位置的话看个人习惯，开始迁下来以后权限是只读的，这使得后面使用IDEA导入时候会出现权限的问题，所以需要修改权限修改权限命令sudo chmod -R 777 &lt;yourfilename&gt;这里简单说一下这个命令 chmod 修改文件和文件夹读写执行属性，777 指的就是二进制编码，总共有3位111就是可读可写可编译，即拥有全权限，可写 w=4可读 r=2可执行 x=1 ，详细介绍chown和chmod区别get !3、Package + Compile这里说一下，我最开始没有跑GeoSpark，而是调的其中的一个案例，如果只是想简单的在本地IDE运行该项目的话，完全不需要这个步骤，然而……我还是照着做了这部分基本没什么问题，只要你maven还有Sbt装好了，按照项目说明一步步执行，就都可以编译通过4、导入项目import project-&gt;GeoSparkTemplateProject/geospark/java-&gt;import project from external model-&gt;maven这里需要注意运行Java部分的代码时，导入项目为maven项目，然后一路next即可，选择SDK时候添加你的jdk1.8 5、build+run/src/main/Example 点击运行，好，报错 123456789101112131415161718192021222324252627282930Exception in thread "main" java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.()V from class org.apache.hadoop.mapred.FileInputFormatat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:312)at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)at scala.Option.getOrElse(Option.scala:121)at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)at scala.Option.getOrElse(Option.scala:121)at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)at scala.Option.getOrElse(Option.scala:121)at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115)at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108)at org.apache.spark.api.java.JavaRDDLike$class.aggregate(JavaRDDLike.scala:426)at org.apache.spark.api.java.AbstractJavaRDDLike.aggregate(JavaRDDLike.scala:45)at org.datasyslab.geospark.spatialRDD.SpatialRDD.analyze(SpatialRDD.java:430)at org.datasyslab.geospark.spatialRDD.SpatialRDD.analyze(SpatialRDD.java:404)at org.datasyslab.geospark.spatialRDD.PointRDD.(PointRDD.java:300)at example.Example.testSpatialRangeQuery(Example.java:156)at example.Example.main(Example.java:128) 仔细看，主要是IllegalAccessError，Google搜索1、https://blog.csdn.net/superzyl/article/details/53764731开始解决，一句话就是项目中的依赖出现了问题，guava冲突，但是在我的pom.xml文件中并没有这个依赖啊？ 无果。。2、http://www.bubuko.com/infodetail-1909839.html还是那个原因，但是给出了一点具体的解决办法查看jar包，发现有多个版本的guava，这部分后面项目开发者都进行了修改，现在看到的只有一个版本 我当时是有4个，然后我具体不知道是哪个版本需要，就保留了16.0.1，把其余的都删了，再重新进行整合，编译，结果还是同样的错误到这里为止，我就不清楚该怎么解决，期间删了项目重新下载，删除其他版本的guava还是没用3、问导师这个时候我知道问题肯定出在依赖上，应该是版本依赖冲突或者缺少依赖的问题，但是不知道具体在哪里导师给我添加了GeoSpark还有Babylon的依赖，这里添加的话就是去maven的官网，对应依赖的地方搜索你想要添加的，就会得到对应的代码，然后复制粘贴到项目的pom.xml文件中即可，这部分我是刚接触还不太明白，后面慢慢熟悉。然而添加以后还是不行，我又开始自己琢磨4、看原项目说明文档这时候重新仔细的又看了一下原项目的说明，按照说的都做了不知道问题出在哪里5、问大佬实在不知道该怎么办，看了issue中也没有这个错误的回答，问了下师兄说需要打包编译吧，我这里比较不确定，索性我直接开了个issue，当时其实抱着问问的心态，压根儿没想着大佬会回复我，因为国内好多博客下面提问，从来没有收到回复=-= 不要嘲笑我蹩脚的英文。。。没过几分钟就收到了回复，他说根本还是Spark依赖冲突的问题，让我理解dependency scope Scope几种模式：1、test范围指的是测试范围有效，在编译和打包时都不会使用这个依赖2、compile范围指的是编译范围有效，在编译和打包时都会将依赖存储进去3、provided依赖：在编译和测试的过程有效，最后生成war包时不会加入，诸如：servlet-api，因为servlet-api，tomcat等web服务器已经存在了，如果再打包会冲突4、runtime在运行的时候依赖，在编译的时候不依赖一般情况下默认是compile我按照他说的又重新安装了IDEA，把我原来的配置文件删除，再重新导入项目，直接运行，还是报错。。。没办法了，只能再继续提问 收到回复赶紧试了一下 ！！！ 总结 最新版的项目只要配置好环境，迁下来运行就可以了说说这个过程，花了很久的时间，虽然最后还是没能自己解决，但是过程确实学到了很多的东西，这只是本地IDE中运行项目，后面需要在Spark cluster运行，到现在很多东西还是很模糊，慢慢一点点学习积累吧！]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>GeoSpark</tag>
        <tag>maven</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android学习系列——ImageView用法]]></title>
    <url>%2F2018%2F01%2F30%2FAndroid%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94ImageView%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[用Android开发也有一段时间，因为是从一开始就做项目，很多东西就是模仿着做，也没有仔细研究，准备把一些开发过程中遇到的需要注意区分的细节写下来，让思路清晰一些，以后用到也可以看看。 ImageView用法 ImageView就是Android中用来显示图片的一个控件 区分属性android:src和android:background设置俩个ImageView，分别用不同属性显示效果如下：可以看出src属性是将图片加载，不会随区域的变化而改变，background是使用图片填充 ImageView 和ImageButton都可以设置点击，用法其实没什么区别，唯一区别就是ImageButton 拥有默认背景，而ImageView 没有，如下所示显示效果可以看到ImageButton是有背景色的另外，通过ImageButton 的类定义我们可以看到ImageButton 继承自ImageView而 ImageView 类又是继承自View 类]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>ImageView</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GithubPages+Hexo搭建个人博客]]></title>
    <url>%2F2018%2F01%2F28%2FGithubPages%2BHexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[如果是有个人服务器的话，建议选择WP（wordpress）搭建个人博客，可以参考这篇文章VPS+LNMP+WordPress搭建个人网站。没有的话，Hexo确实是一个很好的选择，一来简洁大方，十分优雅，二来能够自己折腾，可以设计的地方很多。好，话不多说，接下来详细介绍我搭建个人博客的过程。我的GithubPages+Hexo博客欢迎访问我的Hexo博客 安装环境win10企业版 64位Node.js v8.9.3Git version2.15.0 相关步骤 安装Node.js 步骤1、到Node.js官方网址下载对应版本一般我们下载最新的LTS（长期支持）版本即可，本教程以Version8.9.3对应的msi64bit安装为例，其他版本类似步骤2、双击下载的安装包将出现如下界面：步骤3、一路点击next即可，注意选择安装路径，这个看个人喜好，我一般软件安装在D盘，我这里选择的安装路径是D:\nodejs步骤4、检查是否安装成功win+R快捷键输入cmd1输入 node -v 输入时注意中间的空格即可显示安装的Node.js版本 安装Git 步骤1、到官方网址下载对应的版本Git各平台下载地址：https://git-scm.com/downloadWindows平台Git下载地址：https://git-scm.com/download/win安装过程不需要多说什么，一路下一步即可。安装成功后在桌面或文件夹中点击鼠标右键可以看到Git Bash Here选项。步骤2、检查Git版本 1输入 git --version 输入注意是双- GitHubPages配置 步骤1、进入GitHub官网注册账号，具体注册这里不再赘述。官网连接 https://github.com/步骤2、新建项目注意这里的Repository名字格式一定是账户名.github,io比如我的就是：yabosun.github.io在建好的项目右侧有Settings向下拉可看到GitHub Pages点击对应的网址你会发现该项目已经被部署到网络上，你可以通过外网来访问它。 安装Hexo 在自己认为合适的地方创建一个文件夹，用来存放之后博客的文档以及配置文件，我是在F盘新建了文件夹，并命名为BlogWorkspace。然后点击进入创建的文件夹，点击鼠标右键选择Git Bash Here 1输入 npm install hexo -g 开始安装Hexo 这个过程需要几分钟时间不等，看个人网络状况。安装完成后，检查是否安装成功 1输入 hexo -v 初始化该文件夹1输入 hexo init 这个过程也有点漫长需要等待几分钟，最后出现Start blogging with Hexo！表示安装成功。 开始安装所需要的组件1输入 npm install 首次体验Hexo1输入 hexo g 可以开启本地服务器1输入 hexo s 点击http://localhost:4000/ 可以正式体验hexo，出现Hexo的界面就表示安装成功了 Hexo连接GithubPages 将Hexo与GithubPages联系起来，首次运行的话这里需要设置Git的user name和emailctrl+C结束之前的sever 1输入 git config --global user.name "yabosun" 这里的username为你自己的GitHub用户名 1输入 git config --g global user.email "yabosun@163.com" 这里user.email为你GitHub账户绑定的邮箱 检查是否有.ssh文件1输入 cd ~/.ssh 生成密钥1输入 ssh-keygen -t rsa -C "yabosun@163.com" 连续回车 添加密钥到ssh-agent1输入 eval "$(ssh-agent -s)" 添加生成的SSH key到ssh-agent1输入 ssh-add ~/.ssh/id_rsa 登陆Github，点击头像下的settings，添加ssh新建一个ssh将C:\Users\yabosun\ssh路径下（这里要对应自己电脑的路径）的 id_rsa.pub文件中的内容复制到key 1输入ssh -T git@github.com 注意这里不用改任何名称。 配置Deploy 在你的博客所在文件夹根目录，例如我的是在F:\BlogWorkspace找到_config.yml文件，点击编辑，我是用的SublimeText3打开的，在文件末尾输入： 这里需要注意的是格式一定是：后跟一个空格，是它固定的格式，我一开始就在这里掉了坑，名称对应自己的GitHub项目名称，登陆你的GitHub账户，进入之前创建的Repository，点击Clone or download，选择Clone with SSH并复制就是你的repository 到这里基本上博客已经搭建成功，接下来安装扩展 1输入 npm install hexo-deployer-git --save 使用MarkDown编辑器编写好文章，编辑器最开始可以使用CSDN或者简书等博客内置的编辑器，Windows上推荐使用MarkDownPad2编辑器，不过我用了一下，别的功能还好，如果是要插入图片或者一些高级功能需要升级（付费），所以我现在基本就是在csdn编辑，然后导出MD文件，并上传到Github。另外你也可以直接使用命令生成MD文档： 1输入 hexo n "title" 这里的title是你文章的标题，这个时候你可以看到在F:\BlogWorkspace\source\_posts这个文件夹下会生成你命名的MD文章，打开可以直接编辑，具体的MarkDown语法可以参考这篇文章http://franky47.cn/2018/01/20/How%20to%20use%20MarkdownEditer/#more 生成并部署你的文章到GitHub服务器上1输入 hexo g -d 部署成功后访问你的地址：http://用户名.github.io，即可看到你文章的界面。到这里，基本的Hexo网站已经搭建完成。 参考使用Hexo+Github一步步搭建自己的博客Node.js安装配置]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>GithubPages</tag>
        <tag>Hexo</tag>
        <tag>个人博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前言]]></title>
    <url>%2F2018%2F01%2F27%2F%E5%89%8D%E8%A8%80%2F</url>
    <content type="text"><![CDATA[今天是2018年1月27日，武汉迎来了一场久违的大雪，好久之前就想着搭建博客，写写自己平时的一些心得体会。但是奈何一直沉（mang）迷（yu）学（suo）习（shi），直到上周才断断续续的搭建起最初的版本欢迎访问，后续再慢慢的更新优化。今天也算是博客网站初次搭建好第一次开始写自己的博客，所以还是蛮有纪念意义的。趁着今天这个日子，写一下自己的一些感想，包括对过去半年的总结，以及对2018年的憧憬。 总结过去 认知 日子真的是过的蛮快，经常就是一看日期，卧槽，又周日了？！细细回想，这半年来还是经历了很多。最初读研的想法就是觉得自己在大学阶段实在是没有学到什么，不知道自己怎么去找到一份满意的工作，所以读研的最开始还是怀着一颗努力学习的心的，看各种教学视频，看牛人的博客，制定自己的学习目标，摸索编译器的使用，看别人的学习经验，也是在这个阶段打定主意以后更多的从事跟Java有关的工作，可能也没什么原因，就是简单的觉得看着舒服。 懵逼 跟导师说明自己的想法后，很快给了我一个锻炼的机会，用Java实现一个算法，当时满怀欣喜，但是等到真正下手的时候却不知道该怎么做，所以那段时间也是一直都没啥进展，只是知道算法的流程，真的写代码时候一脸懵逼，写代码我觉得最初还是要有看的见的东西，才能提起兴趣，就这样很快半个多月就过去了，在这里要特别的感谢自己研究生的导师，并不是像别人说的研究生导师与学生是老板与员工的关系，老师真的很认真的给我讲解，甚至亲手教我一行一行的怎么实现一个简单的算法，蓝而，我最后还是没能把那个算法写出来。 迷茫 就这样到了正式开学，我还是脑子一片迷茫，进入实验室项目组，看了一段时间项目的代码，等到写月底总结的时候，突然发现，自己啥新的功能也没有实现，交代的任务也是做的一塌糊涂，真的毫无成就感，很多时候甚至怀疑自己是不是不适合敲代码，不懂该怎么下手，感觉东西学了很快就忘。还有就是本身对MFC实在提不起兴趣。 新奇 再到后来就没有做公司的项目，跑去跟导师做事情，这个过程也是收获蛮多的，学会了使用Github？SVN？好吧，你会说这些不算，老师让我去学了一下一款轻量级数据库SQlite，学了一些语法，然后就开始着手将已有的地质数据导入到这个数据库，当然这个时候实现都是用的C++，简单说，就是模仿老师写的代码，实现基本相似的功能，好，终于有了一些进展，看到自己成功把数据一行一行用代码导入数据库心里还是蛮开心的，但是，这个乱码是什么鬼！就不能给我一个完美的机会？当然在老师的帮助下修改了，后面就做了一段时间的小蜜蜂，哦不，砖瓦工更合适些。一天老师把我叫去，说这段时间你也模仿实现了一些，接下来就自己写，你实现一下这个功能，生成一个断层面，what？不懂。虽然嘴上嗯，心里还是没太搞明白。 磨练 差不多就到了快10月，后面就是自己的兼职实习阶段，感谢威总带我，走上了Android开发之路，去公司还是跟实验室不一样的，公司催的很紧，你必须得出东西，所以压力还是有点的，仗着脸皮厚，有时候没写出来还是拖了些天才提交，代码质量自然不用多说，现在回头看看之前写的，有点想打死自己，虽然被疯狂怼还是努力的做，也感觉自己收获很大，至少能看别人的代码，自己照着实现，然后能调通这个功能了，虽然经常因为命名混乱，考虑不周被怼，但是还是有些成就感，学到了很多Android基础的编程，这部分会在后面的博客中详细记录。 致谢 首先要感谢我家小七，今天也是相恋537天，感谢一路的陪伴，感谢对我的照顾。再次就是感谢遇到自己现在的导师，感谢对我的包容，对我的细心指导。最想感谢的还有就是威总，在我编程学习阶段给我很多的帮助，帮我调bug，虽然每次一脸嫌弃哈哈。还有就是感谢公司，感谢老王，让我看到自己身上最大的不足，就是学一个东西，没有认真的弄明白，只是简单的去模仿，没有真正的搞懂。也感谢遇到133的一群机电大佬（不是基佬），一起开黑，一起上分。OK，矫情的话就说到这。 展望未来2018简单计划： 旅游 争取抽时间出去旅游2次，目前考虑的城市有西安、成都、杭州、上海，具体待定，我带上钱，你带上我@王七宝 学习 深入学习Android，包括安卓的一些优化学习Java，多线程编程、网络编程学习web开发相关，包括JS、HTML等学习python，具体还没有了解，涉及到深度学习，人工智能学习在Linux系统编程开发扎实计算机基础，抽时间把计算机网络、计算机操作系统、数据结构等计算机基础课程学习一下，通过牛客网编程至少通过100 当然最大需要改变的就是以后学习东西要努力去深入了解，不要还是走马观花。想法总是美好的，现实可能很残酷，希望等到2019年看这篇博客时能没那么多后悔，而更多的是收获满满。加油！]]></content>
      <categories>
        <category>Hello World</category>
      </categories>
      <tags>
        <tag>日常随笔</tag>
      </tags>
  </entry>
</search>
